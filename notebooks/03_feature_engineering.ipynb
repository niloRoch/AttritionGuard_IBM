{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9ef8c4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Pr√©-processamento e Feature Engineering\n",
    "# Employee Attrition Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class AttritionPreprocessor:\n",
    "    \"\"\"Classe para pr√©-processamento completo dos dados de attrition\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_names = None\n",
    "        self.target_encoder = LabelEncoder()\n",
    "        \n",
    "    def load_and_initial_analysis(self, filepath):\n",
    "        \"\"\"Carrega dados e faz an√°lise inicial\"\"\"\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(\"üîÑ PR√â-PROCESSAMENTO E FEATURE ENGINEERING\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Carregar dados\n",
    "        self.df = pd.read_csv(filepath)\n",
    "        print(f\"\\nüìä Dataset carregado: {self.df.shape[0]} linhas, {self.df.shape[1]} colunas\")\n",
    "        \n",
    "        # An√°lise inicial\n",
    "        print(f\"\\nüîç An√°lise de qualidade:\")\n",
    "        print(f\"- Valores ausentes: {self.df.isnull().sum().sum()}\")\n",
    "        print(f\"- Duplicatas: {self.df.duplicated().sum()}\")\n",
    "        \n",
    "        # Identificar tipos de vari√°veis\n",
    "        self.numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        self.categorical_cols = self.df.select_dtypes(include=['object']).columns.tolist()\n",
    "        \n",
    "        print(f\"- Vari√°veis num√©ricas: {len(self.numeric_cols)}\")\n",
    "        print(f\"- Vari√°veis categ√≥ricas: {len(self.categorical_cols)}\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def remove_redundant_columns(self):\n",
    "        \"\"\"Remove colunas redundantes ou invari√°veis\"\"\"\n",
    "        \n",
    "        print(f\"\\nüóëÔ∏è REMO√á√ÉO DE COLUNAS REDUNDANTES:\")\n",
    "        \n",
    "        # Identificar colunas com valores √∫nicos\n",
    "        constant_cols = [col for col in self.df.columns if self.df[col].nunique() == 1]\n",
    "        \n",
    "        # Colunas espec√≠ficas a remover\n",
    "        columns_to_remove = ['EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours']\n",
    "        columns_to_remove.extend(constant_cols)\n",
    "        columns_to_remove = list(set(columns_to_remove))  # Remove duplicatas\n",
    "        \n",
    "        # Verificar se existem no dataset\n",
    "        existing_cols_to_remove = [col for col in columns_to_remove if col in self.df.columns]\n",
    "        \n",
    "        print(f\"Removendo colunas: {existing_cols_to_remove}\")\n",
    "        \n",
    "        self.df = self.df.drop(columns=existing_cols_to_remove)\n",
    "        \n",
    "        # Atualizar listas de colunas\n",
    "        self.numeric_cols = [col for col in self.numeric_cols if col in self.df.columns]\n",
    "        self.categorical_cols = [col for col in self.categorical_cols if col in self.df.columns]\n",
    "        \n",
    "        print(f\"‚úÖ Dataset ap√≥s remo√ß√£o: {self.df.shape}\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def create_new_features(self):\n",
    "        \"\"\"Cria novas features atrav√©s de feature engineering\"\"\"\n",
    "        \n",
    "        print(f\"\\nüõ†Ô∏è FEATURE ENGINEERING:\")\n",
    "        \n",
    "        # 1. Faixas et√°rias\n",
    "        self.df['AgeGroup'] = pd.cut(self.df['Age'], \n",
    "                                    bins=[0, 25, 35, 45, 55, 100], \n",
    "                                    labels=['Young', 'Adult', 'Middle', 'Senior', 'Elder'])\n",
    "        \n",
    "        # 2. Faixas salariais\n",
    "        salary_quartiles = self.df['MonthlyIncome'].quantile([0.25, 0.5, 0.75])\n",
    "        self.df['SalaryLevel'] = pd.cut(self.df['MonthlyIncome'], \n",
    "                                       bins=[0, salary_quartiles[0.25], salary_quartiles[0.5], \n",
    "                                            salary_quartiles[0.75], float('inf')],\n",
    "                                       labels=['Low', 'Medium', 'High', 'VeryHigh'])\n",
    "        \n",
    "        # 3. Taxa de crescimento salarial anualizada\n",
    "        self.df['SalaryGrowthRate'] = (self.df['PercentSalaryHike'] / 100) * self.df['MonthlyIncome']\n",
    "        \n",
    "        # 4. Propor√ß√£o de anos na empresa vs idade\n",
    "        self.df['CompanyTenureRatio'] = self.df['YearsAtCompany'] / (self.df['Age'] - 18 + 1)\n",
    "        \n",
    "        # 5. Experi√™ncia total vs anos na empresa atual\n",
    "        self.df['ExperienceRatio'] = self.df['YearsAtCompany'] / (self.df['TotalWorkingYears'] + 1)\n",
    "        \n",
    "        # 6. Indicador de promo√ß√£o recente (√∫ltimo ano)\n",
    "        self.df['RecentPromotion'] = (self.df['YearsSinceLastPromotion'] <= 1).astype(int)\n",
    "        \n",
    "        # 7. Estabilidade de carreira (poucas empresas vs muita experi√™ncia)\n",
    "        self.df['CareerStability'] = self.df['TotalWorkingYears'] / (self.df['NumCompaniesWorked'] + 1)\n",
    "        \n",
    "        # 8. Score de satisfa√ß√£o geral (m√©dia das satisfa√ß√µes)\n",
    "        satisfaction_cols = ['JobSatisfaction', 'EnvironmentSatisfaction', 'RelationshipSatisfaction']\n",
    "        if all(col in self.df.columns for col in satisfaction_cols):\n",
    "            self.df['OverallSatisfaction'] = self.df[satisfaction_cols].mean(axis=1)\n",
    "        \n",
    "        # 9. Indicador de alto performer (alta performance + alta satisfa√ß√£o)\n",
    "        if 'PerformanceRating' in self.df.columns:\n",
    "            self.df['HighPerformer'] = ((self.df['PerformanceRating'] >= 3) & \n",
    "                                       (self.df['JobSatisfaction'] >= 3)).astype(int)\n",
    "        \n",
    "        # 10. Dist√¢ncia categorizada\n",
    "        distance_median = self.df['DistanceFromHome'].median()\n",
    "        self.df['DistanceCategory'] = np.where(self.df['DistanceFromHome'] > distance_median, \n",
    "                                              'Far', 'Near')\n",
    "        \n",
    "        # 11. Indicador de workaholic (overtime + high involvement)\n",
    "        if 'JobInvolvement' in self.df.columns:\n",
    "            self.df['Workaholic'] = ((self.df['OverTime'] == 'Yes') & \n",
    "                                    (self.df['JobInvolvement'] >= 3)).astype(int)\n",
    "        \n",
    "        print(f\"‚úÖ Criadas {len(self.df.columns) - len(self.numeric_cols) - len(self.categorical_cols)} novas features\")\n",
    "        \n",
    "        # Atualizar listas de colunas\n",
    "        self.numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        self.categorical_cols = self.df.select_dtypes(include=['object']).columns.tolist()\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def handle_categorical_variables(self):\n",
    "        \"\"\"Processa vari√°veis categ√≥ricas\"\"\"\n",
    "        \n",
    "        print(f\"\\nüè∑Ô∏è PROCESSAMENTO DE VARI√ÅVEIS CATEG√ìRICAS:\")\n",
    "        \n",
    "        # Separar target das outras categ√≥ricas\n",
    "        categorical_features = [col for col in self.categorical_cols if col != 'Attrition']\n",
    "        \n",
    "        print(f\"Vari√°veis categ√≥ricas a processar: {len(categorical_features)}\")\n",
    "        \n",
    "        # Encoding do target\n",
    "        self.df['Attrition_Binary'] = self.target_encoder.fit_transform(self.df['Attrition'])\n",
    "        \n",
    "        # Para vari√°veis com muitas categorias, usar Target Encoding\n",
    "        high_cardinality_cols = [col for col in categorical_features \n",
    "                               if self.df[col].nunique() > 5]\n",
    "        \n",
    "        # Para vari√°veis com poucas categorias, usar One-Hot Encoding\n",
    "        low_cardinality_cols = [col for col in categorical_features \n",
    "                              if self.df[col].nunique() <= 5]\n",
    "        \n",
    "        print(f\"- Alta cardinalidade (Target Encoding): {high_cardinality_cols}\")\n",
    "        print(f\"- Baixa cardinalidade (One-Hot): {low_cardinality_cols}\")\n",
    "        \n",
    "        # Target Encoding para alta cardinalidade\n",
    "        for col in high_cardinality_cols:\n",
    "            target_mean = self.df.groupby(col)['Attrition_Binary'].mean()\n",
    "            self.df[f'{col}_TargetEnc'] = self.df[col].map(target_mean)\n",
    "        \n",
    "        # One-Hot Encoding para baixa cardinalidade\n",
    "        for col in low_cardinality_cols:\n",
    "            dummies = pd.get_dummies(self.df[col], prefix=col, drop_first=True)\n",
    "            self.df = pd.concat([self.df, dummies], axis=1)\n",
    "        \n",
    "        # Remover colunas categ√≥ricas originais (exceto target)\n",
    "        cols_to_drop = [col for col in categorical_features]\n",
    "        self.df = self.df.drop(columns=cols_to_drop)\n",
    "        \n",
    "        print(f\"‚úÖ Processamento categ√≥rico conclu√≠do. Shape: {self.df.shape}\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def handle_outliers(self, method='iqr', threshold=3):\n",
    "        \"\"\"Trata outliers nas vari√°veis num√©ricas\"\"\"\n",
    "        \n",
    "        print(f\"\\nüéØ TRATAMENTO DE OUTLIERS (m√©todo: {method}):\")\n",
    "        \n",
    "        numeric_features = [col for col in self.df.select_dtypes(include=[np.number]).columns \n",
    "                           if col not in ['Attrition_Binary']]\n",
    "        \n",
    "        outliers_info = {}\n",
    "        \n",
    "        for col in numeric_features:\n",
    "            if method == 'iqr':\n",
    "                Q1 = self.df[col].quantile(0.25)\n",
    "                Q3 = self.df[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                outliers_mask = (self.df[col] < lower_bound) | (self.df[col] > upper_bound)\n",
    "                \n",
    "            elif method == 'zscore':\n",
    "                z_scores = np.abs((self.df[col] - self.df[col].mean()) / self.df[col].std())\n",
    "                outliers_mask = z_scores > threshold\n",
    "            \n",
    "            outliers_count = outliers_mask.sum()\n",
    "            outliers_info[col] = outliers_count\n",
    "            \n",
    "            # Cap outliers (winsorizing) em vez de remover\n",
    "            if outliers_count > 0:\n",
    "                if method == 'iqr':\n",
    "                    self.df[col] = np.clip(self.df[col], lower_bound, upper_bound)\n",
    "                elif method == 'zscore':\n",
    "                    self.df[col] = np.clip(self.df[col], \n",
    "                                          self.df[col].quantile(0.01), \n",
    "                                          self.df[col].quantile(0.99))\n",
    "        \n",
    "        # Mostrar estat√≠sticas\n",
    "        total_outliers = sum(outliers_info.values())\n",
    "        print(f\"Total de outliers identificados: {total_outliers}\")\n",
    "        \n",
    "        if total_outliers > 0:\n",
    "            print(\"Top 5 vari√°veis com mais outliers:\")\n",
    "            sorted_outliers = sorted(outliers_info.items(), key=lambda x: x[1], reverse=True)\n",
    "            for col, count in sorted_outliers[:5]:\n",
    "                if count > 0:\n",
    "                    print(f\"  {col}: {count}\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def scale_features(self):\n",
    "        \"\"\"Normaliza features num√©ricas\"\"\"\n",
    "        \n",
    "        print(f\"\\nüìè NORMALIZA√á√ÉO DE FEATURES:\")\n",
    "        \n",
    "        # Identificar features num√©ricas (exceto target)\n",
    "        numeric_features = [col for col in self.df.select_dtypes(include=[np.number]).columns \n",
    "                           if col not in ['Attrition_Binary', 'Attrition']]\n",
    "        \n",
    "        print(f\"Normalizando {len(numeric_features)} features num√©ricas\")\n",
    "        \n",
    "        # Aplicar StandardScaler\n",
    "        self.df[numeric_features] = self.scaler.fit_transform(self.df[numeric_features])\n",
    "        \n",
    "        print(f\"‚úÖ Normaliza√ß√£o conclu√≠da\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def select_best_features(self, k=20):\n",
    "        \"\"\"Seleciona as melhores features usando m√©todos estat√≠sticos\"\"\"\n",
    "        \n",
    "        print(f\"\\nüéØ SELE√á√ÉO DE FEATURES (k={k}):\")\n",
    "        \n",
    "        # Preparar dados\n",
    "        X = self.df.drop(['Attrition', 'Attrition_Binary'], axis=1, errors='ignore')\n",
    "        y = self.df['Attrition_Binary']\n",
    "        \n",
    "        # Aplicar SelectKBest com f_classif\n",
    "        selector = SelectKBest(score_func=f_classif, k=k)\n",
    "        X_selected = selector.fit_transform(X, y)\n",
    "        \n",
    "        # Obter nomes das features selecionadas\n",
    "        selected_features = X.columns[selector.get_support()].tolist()\n",
    "        feature_scores = selector.scores_[selector.get_support()]\n",
    "        \n",
    "        # Criar DataFrame com scores\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': selected_features,\n",
    "            'Score': feature_scores\n",
    "        }).sort_values('Score', ascending=False)\n",
    "        \n",
    "        print(\"üèÜ TOP 10 FEATURES SELECIONADAS:\")\n",
    "        print(feature_importance.head(10))\n",
    "        \n",
    "        # Manter apenas features selecionadas + target\n",
    "        self.df_selected = self.df[selected_features + ['Attrition_Binary']].copy()\n",
    "        self.feature_names = selected_features\n",
    "        \n",
    "        print(f\"\\n‚úÖ Dataset final: {self.df_selected.shape}\")\n",
    "        \n",
    "        return self.df_selected, feature_importance\n",
    "    \n",
    "    def split_data(self, test_size=0.2, val_size=0.1):\n",
    "        \"\"\"Divide dados em train/validation/test\"\"\"\n",
    "        \n",
    "        print(f\"\\nüîÑ DIVIS√ÉO DOS DADOS:\")\n",
    "        print(f\"Train: {1-test_size-val_size:.1%} | Validation: {val_size:.1%} | Test: {test_size:.1%}\")\n",
    "        \n",
    "        # Preparar X e y\n",
    "        X = self.df_selected.drop('Attrition_Binary', axis=1)\n",
    "        y = self.df_selected['Attrition_Binary']\n",
    "        \n",
    "        # Primeira divis√£o: train+val vs test\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, stratify=y, random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # Segunda divis√£o: train vs validation\n",
    "        if val_size > 0:\n",
    "            val_size_adjusted = val_size / (1 - test_size)\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X_temp, y_temp, test_size=val_size_adjusted, stratify=y_temp, \n",
    "                random_state=self.random_state\n",
    "            )\n",
    "        else:\n",
    "            X_train, X_val, y_train, y_val = X_temp, None, y_temp, None\n",
    "        \n",
    "        print(f\"‚úÖ Divis√£o conclu√≠da:\")\n",
    "        print(f\"  Train: {X_train.shape}\")\n",
    "        if X_val is not None:\n",
    "            print(f\"  Validation: {X_val.shape}\")\n",
    "        print(f\"  Test: {X_test.shape}\")\n",
    "        \n",
    "        # Verificar balanceamento\n",
    "        print(f\"\\nüìä Distribui√ß√£o do target:\")\n",
    "        print(f\"  Train: {y_train.mean():.3f}\")\n",
    "        if y_val is not None:\n",
    "            print(f\"  Validation: {y_val.mean():.3f}\")\n",
    "        print(f\"  Test: {y_test.mean():.3f}\")\n",
    "        \n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \n",
    "    def full_preprocessing_pipeline(self, filepath):\n",
    "        \"\"\"Pipeline completo de pr√©-processamento\"\"\"\n",
    "        \n",
    "        print(\"üöÄ EXECUTANDO PIPELINE COMPLETO DE PR√â-PROCESSAMENTO\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # 1. Carregar dados\n",
    "        self.load_and_initial_analysis(filepath)\n",
    "        \n",
    "        # 2. Remover colunas redundantes\n",
    "        self.remove_redundant_columns()\n",
    "        \n",
    "        # 3. Feature engineering\n",
    "        self.create_new_features()\n",
    "        \n",
    "        # 4. Processar categ√≥ricas\n",
    "        self.handle_categorical_variables()\n",
    "        \n",
    "        # 5. Tratar outliers\n",
    "        self.handle_outliers()\n",
    "        \n",
    "        # 6. Normalizar features\n",
    "        self.scale_features()\n",
    "        \n",
    "        # 7. Selecionar features\n",
    "        df_final, feature_importance = self.select_best_features()\n",
    "        \n",
    "        # 8. Dividir dados\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = self.split_data()\n",
    "        \n",
    "        print(\"\\nüéâ PR√â-PROCESSAMENTO CONCLU√çDO COM SUCESSO!\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Retornar dados processados\n",
    "        return {\n",
    "            'X_train': X_train, 'X_val': X_val, 'X_test': X_test,\n",
    "            'y_train': y_train, 'y_val': y_val, 'y_test': y_test,\n",
    "            'feature_importance': feature_importance,\n",
    "            'preprocessor': self\n",
    "        }\n",
    "    \n",
    "    def save_processed_data(self, results, output_dir='processed_data/'):\n",
    "        \"\"\"Salva dados processados\"\"\"\n",
    "        \n",
    "        import os\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Salvar datasets\n",
    "        results['X_train'].to_csv(f'{output_dir}X_train.csv', index=False)\n",
    "        results['X_test'].to_csv(f'{output_dir}X_test.csv', index=False)\n",
    "        results['y_train'].to_csv(f'{output_dir}y_train.csv', index=False)\n",
    "        results['y_test'].to_csv(f'{output_dir}y_test.csv', index=False)\n",
    "        \n",
    "        if results['X_val'] is not None:\n",
    "            results['X_val'].to_csv(f'{output_dir}X_val.csv', index=False)\n",
    "            results['y_val'].to_csv(f'{output_dir}y_val.csv', index=False)\n",
    "        \n",
    "        # Salvar import√¢ncias\n",
    "        results['feature_importance'].to_csv(f'{output_dir}feature_importance.csv', index=False)\n",
    "        \n",
    "        print(f\"üíæ Dados processados salvos em: {output_dir}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# EXEMPLO DE USO\n",
    "# ========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Inicializar preprocessador\n",
    "    preprocessor = AttritionPreprocessor(random_state=42)\n",
    "    \n",
    "    # Executar pipeline completo\n",
    "    results = preprocessor.full_preprocessing_pipeline('IBM_Fn-UseC_-HR-Employee-Attrition.csv')\n",
    "    \n",
    "    # Salvar resultados\n",
    "    preprocessor.save_processed_data(results)\n",
    "    \n",
    "    print(\"\\nüìù RESUMO DOS DADOS PROCESSADOS:\")\n",
    "    print(f\"Features selecionadas: {len(results['feature_importance'])}\")\n",
    "    print(f\"Tamanho do conjunto de treino: {results['X_train'].shape}\")\n",
    "    print(f\"Taxa de attrition no treino: {results['y_train'].mean():.3f}\")\n",
    "    \n",
    "    print(\"\\nüéØ PR√ìXIMOS PASSOS:\")\n",
    "    print(\"1. Executar modelagem com diferentes algoritmos\")\n",
    "    print(\"2. Otimizar hiperpar√¢metros\")\n",
    "    print(\"3. Avaliar interpretabilidade com SHAP\")\n",
    "    print(\"4. Criar dashboard interativo\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
