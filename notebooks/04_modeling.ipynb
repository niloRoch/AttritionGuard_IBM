{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8145f2bf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Modelagem Machine Learning - Employee Attrition\n",
    "# Compara√ß√£o de algoritmos e otimiza√ß√£o\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, \n",
    "                           roc_curve, precision_recall_curve, accuracy_score, \n",
    "                           precision_score, recall_score, f1_score)\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class AttritionModelingPipeline:\n",
    "    \"\"\"Pipeline completo para modelagem de Employee Attrition\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.best_models = {}\n",
    "        self.results = {}\n",
    "        self.cv_scores = {}\n",
    "        \n",
    "    def load_processed_data(self, data_dir='processed_data/'):\n",
    "        \"\"\"Carrega dados pr√©-processados\"\"\"\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(\"ü§ñ MODELAGEM MACHINE LEARNING - EMPLOYEE ATTRITION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            self.X_train = pd.read_csv(f'{data_dir}X_train.csv')\n",
    "            self.X_test = pd.read_csv(f'{data_dir}X_test.csv')\n",
    "            self.y_train = pd.read_csv(f'{data_dir}y_train.csv').squeeze()\n",
    "            self.y_test = pd.read_csv(f'{data_dir}y_test.csv').squeeze()\n",
    "            \n",
    "            # Tentar carregar validation se existir\n",
    "            try:\n",
    "                self.X_val = pd.read_csv(f'{data_dir}X_val.csv')\n",
    "                self.y_val = pd.read_csv(f'{data_dir}y_val.csv').squeeze()\n",
    "                print(f\"‚úÖ Validation set carregado: {self.X_val.shape}\")\n",
    "            except:\n",
    "                self.X_val = None\n",
    "                self.y_val = None\n",
    "                print(\"‚ÑπÔ∏è Validation set n√£o encontrado\")\n",
    "            \n",
    "            print(f\"‚úÖ Dados carregados:\")\n",
    "            print(f\"  Train: {self.X_train.shape}\")\n",
    "            print(f\"  Test: {self.X_test.shape}\")\n",
    "            print(f\"  Features: {self.X_train.shape[1]}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao carregar dados: {e}\")\n",
    "            print(\"üí° Execute primeiro o pr√©-processamento!\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def initialize_models(self):\n",
    "        \"\"\"Inicializa diferentes modelos para compara√ß√£o\"\"\"\n",
    "        \n",
    "        print(f\"\\nüîß INICIALIZANDO MODELOS:\")\n",
    "        \n",
    "        self.models = {\n",
    "            'Logistic Regression': LogisticRegression(random_state=self.random_state, max_iter=1000),\n",
    "            'Random Forest': RandomForestClassifier(random_state=self.random_state, n_estimators=100),\n",
    "            'XGBoost': xgb.XGBClassifier(random_state=self.random_state, eval_metric='logloss'),\n",
    "            'SVM': SVC(random_state=self.random_state, probability=True),\n",
    "            'Gradient Boosting': GradientBoostingClassifier(random_state=self.random_state),\n",
    "            'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "            'Naive Bayes': GaussianNB()\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ {len(self.models)} modelos inicializados:\")\n",
    "        for name in self.models.keys():\n",
    "            print(f\"  - {name}\")\n",
    "    \n",
    "    def evaluate_baseline_models(self, cv_folds=5):\n",
    "        \"\"\"Avalia modelos baseline com cross-validation\"\"\"\n",
    "        \n",
    "        print(f\"\\nüìä AVALIA√á√ÉO DE MODELOS BASELINE (CV={cv_folds}):\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "        self.cv_scores = {metric: {} for metric in metrics}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            print(f\"\\nüîÑ Avaliando {name}...\")\n",
    "            \n",
    "            # Cross-validation para cada m√©trica\n",
    "            for metric in metrics:\n",
    "                scores = cross_val_score(model, self.X_train, self.y_train, \n",
    "                                       cv=cv, scoring=metric, n_jobs=-1)\n",
    "                self.cv_scores[metric][name] = scores\n",
    "                \n",
    "                print(f\"  {metric.upper()}: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n",
    "        \n",
    "        # Criar DataFrame com resultados\n",
    "        results_list = []\n",
    "        for name in self.models.keys():\n",
    "            row = {'Model': name}\n",
    "            for metric in metrics:\n",
    "                scores = self.cv_scores[metric][name]\n",
    "                row[f'{metric.capitalize()}_Mean'] = scores.mean()\n",
    "                row[f'{metric.capitalize()}_Std'] = scores.std()\n",
    "            results_list.append(row)\n",
    "        \n",
    "        self.baseline_results = pd.DataFrame(results_list)\n",
    "        \n",
    "        print(f\"\\nüìà RANKING DOS MODELOS (por AUC-ROC):\")\n",
    "        ranking = self.baseline_results.sort_values('Roc_auc_Mean', ascending=False)\n",
    "        print(ranking[['Model', 'Accuracy_Mean', 'Precision_Mean', 'Recall_Mean', 'F1_Mean', 'Roc_auc_Mean']].round(3))\n",
    "        \n",
    "        return self.baseline_results\n",
    "    \n",
    "    def hyperparameter_tuning(self, top_models=3):\n",
    "        \"\"\"Otimiza√ß√£o de hiperpar√¢metros para os melhores modelos\"\"\"\n",
    "        \n",
    "        print(f\"\\nüéØ OTIMIZA√á√ÉO DE HIPERPAR√ÇMETROS:\")\n",
    "        \n",
    "        # Selecionar top modelos\n",
    "        top_model_names = (self.baseline_results\n",
    "                          .sort_values('Roc_auc_Mean', ascending=False)\n",
    "                          .head(top_models)['Model'].tolist())\n",
    "        \n",
    "        print(f\"Otimizando top {top_models} modelos: {top_model_names}\")\n",
    "        \n",
    "        # Definir grids de hiperpar√¢metros\n",
    "        param_grids = {\n",
    "            'Logistic Regression': {\n",
    "                'C': [0.1, 1, 10, 100],\n",
    "                'penalty': ['l1', 'l2'],\n",
    "                'solver': ['liblinear']\n",
    "            },\n",
    "            'Random Forest': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [10, 20, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            },\n",
    "            'XGBoost': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [3, 6, 9],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'subsample': [0.8, 0.9, 1.0]\n",
    "            },\n",
    "            'SVM': {\n",
    "                'C': [0.1, 1, 10, 100],\n",
    "                'gamma': ['scale', 'auto', 0.001, 0.01],\n",
    "                'kernel': ['rbf', 'poly']\n",
    "            },\n",
    "            'Gradient Boosting': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'learning_rate': [0.01, 0.1, 0.2]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        for model_name in top_model_names:\n",
    "            if model_name in param_grids:\n",
    "                print(f\"\\nüîç Otimizando {model_name}...\")\n",
    "                \n",
    "                model = self.models[model_name]\n",
    "                param_grid = param_grids[model_name]\n",
    "                \n",
    "                # Grid Search\n",
    "                grid_search = GridSearchCV(\n",
    "                    model, param_grid, cv=cv, scoring='roc_auc', \n",
    "                    n_jobs=-1, verbose=0\n",
    "                )\n",
    "                \n",
    "                grid_search.fit(self.X_train, self.y_train)\n",
    "                \n",
    "                self.best_models[model_name] = grid_search.best_estimator_\n",
    "                \n",
    "                print(f\"  ‚úÖ Melhores par√¢metros: {grid_search.best_params_}\")\n",
    "                print(f\"  ‚úÖ Melhor score: {grid_search.best_score_:.3f}\")\n",
    "            else:\n",
    "                # Para modelos sem grid, usar modelo padr√£o\n",
    "                self.best_models[model_name] = self.models[model_name]\n",
    "        \n",
    "        print(f\"\\nüéâ Otimiza√ß√£o conclu√≠da para {len(self.best_models)} modelos\")\n",
    "    \n",
    "    def evaluate_final_models(self):\n",
    "        \"\"\"Avalia modelos otimizados no conjunto de teste\"\"\"\n",
    "        \n",
    "        print(f\"\\nüèÜ AVALIA√á√ÉO FINAL DOS MODELOS:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        self.final_results = {}\n",
    "        \n",
    "        for name, model in self.best_models.items():\n",
    "            print(f\"\\nüìä Avaliando {name}...\")\n",
    "            \n",
    "            # Treinar modelo\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            \n",
    "            # Predi√ß√µes\n",
    "            y_pred = model.predict(self.X_test)\n",
    "            y_pred_proba = model.predict_proba(self.X_test)[:, 1]\n",
    "            \n",
    "            # Calcular m√©tricas\n",
    "            metrics = {\n",
    "                'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "                'precision': precision_score(self.y_test, y_pred),\n",
    "                'recall': recall_score(self.y_test, y_pred),\n",
    "                'f1': f1_score(self.y_test, y_pred),\n",
    "                'roc_auc': roc_auc_score(self.y_test, y_pred_proba)\n",
    "            }\n",
    "            \n",
    "            self.final_results[name] = {\n",
    "                'model': model,\n",
    "                'predictions': y_pred,\n",
    "                'probabilities': y_pred_proba,\n",
    "                'metrics': metrics\n",
    "            }\n",
    "            \n",
    "            # Mostrar m√©tricas\n",
    "            for metric, value in metrics.items():\n",
    "                print(f\"  {metric.upper()}: {value:.3f}\")\n",
    "        \n",
    "        # Criar DataFrame com resultados finais\n",
    "        final_df = pd.DataFrame({\n",
    "            name: result['metrics'] for name, result in self.final_results.items()\n",
    "        }).T\n",
    "        \n",
    "        print(f\"\\nü•á RANKING FINAL DOS MODELOS:\")\n",
    "        print(final_df.sort_values('roc_auc', ascending=False).round(3))\n",
    "        \n",
    "        return final_df\n",
    "    \n",
    "    def create_model_comparison_plots(self):\n",
    "        \"\"\"Cria visualiza√ß√µes comparativas dos modelos\"\"\"\n",
    "        \n",
    "        print(f\"\\nüìä CRIANDO VISUALIZA√á√ïES COMPARATIVAS:\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "        fig.suptitle('ü§ñ COMPARA√á√ÉO DE MODELOS - EMPLOYEE ATTRITION', fontsize=20, fontweight='bold')\n",
    "        \n",
    "        # 1. Compara√ß√£o de m√©tricas\n",
    "        ax1 = axes[0, 0]\n",
    "        metrics_df = pd.DataFrame({\n",
    "            name: result['metrics'] for name, result in self.final_results.items()\n",
    "        }).T\n",
    "        metrics_df[['accuracy', 'precision', 'recall', 'f1']].plot(kind='bar', ax=ax1, alpha=0.8)\n",
    "        ax1.set_title('M√©tricas por Modelo', fontsize=14, fontweight='bold')\n",
    "        ax1.set_ylabel('Score')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 2. ROC Curves\n",
    "        ax2 = axes[0, 1]\n",
    "        colors = plt.cm.Set1(np.linspace(0, 1, len(self.final_results)))\n",
    "        \n",
    "        for (name, result), color in zip(self.final_results.items(), colors):\n",
    "            fpr, tpr, _ = roc_curve(self.y_test, result['probabilities'])\n",
    "            auc_score = result['metrics']['roc_auc']\n",
    "            ax2.plot(fpr, tpr, color=color, label=f'{name} (AUC = {auc_score:.3f})', linewidth=2)\n",
    "        \n",
    "        ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "        ax2.set_xlabel('False Positive Rate')\n",
    "        ax2.set_ylabel('True Positive Rate')\n",
    "        ax2.set_title('ROC Curves Comparison', fontsize=14, fontweight='bold')\n",
    "        ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax2.grid(alpha=0.3)\n",
    "        \n",
    "        # 3. Precision-Recall Curves\n",
    "        ax3 = axes[0, 2]\n",
    "        \n",
    "        for (name, result), color in zip(self.final_results.items(), colors):\n",
    "            precision, recall, _ = precision_recall_curve(self.y_test, result['probabilities'])\n",
    "            ax3.plot(recall, precision, color=color, label=name, linewidth=2)\n",
    "        \n",
    "        ax3.set_xlabel('Recall')\n",
    "        ax3.set_ylabel('Precision')\n",
    "        ax3.set_title('Precision-Recall Curves', fontsize=14, fontweight='bold')\n",
    "        ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax3.grid(alpha=0.3)\n",
    "        \n",
    "        # 4. Matriz de Confus√£o do Melhor Modelo\n",
    "        best_model_name = metrics_df.sort_values('roc_auc', ascending=False).index[0]\n",
    "        best_predictions = self.final_results[best_model_name]['predictions']\n",
    "        \n",
    "        ax4 = axes[1, 0]\n",
    "        cm = confusion_matrix(self.y_test, best_predictions)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax4)\n",
    "        ax4.set_title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "        ax4.set_ylabel('Actual')\n",
    "        ax4.set_xlabel('Predicted')\n",
    "        \n",
    "        # 5. Distribui√ß√£o de Probabilidades\n",
    "        ax5 = axes[1, 1]\n",
    "        \n",
    "        best_probabilities = self.final_results[best_model_name]['probabilities']\n",
    "        \n",
    "        # Separar por classe real\n",
    "        prob_class_0 = best_probabilities[self.y_test == 0]\n",
    "        prob_class_1 = best_probabilities[self.y_test == 1]\n",
    "        \n",
    "        ax5.hist(prob_class_0, bins=30, alpha=0.7, label='No Attrition', color='blue', density=True)\n",
    "        ax5.hist(prob_class_1, bins=30, alpha=0.7, label='Attrition', color='red', density=True)\n",
    "        ax5.set_xlabel('Predicted Probability')\n",
    "        ax5.set_ylabel('Density')\n",
    "        ax5.set_title(f'Probability Distribution - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "        ax5.legend()\n",
    "        ax5.grid(alpha=0.3)\n",
    "        \n",
    "        # 6. Cross-Validation Scores\n",
    "        ax6 = axes[1, 2]\n",
    "        \n",
    "        if hasattr(self, 'cv_scores'):\n",
    "            cv_auc_scores = pd.DataFrame(self.cv_scores['roc_auc'])\n",
    "            cv_auc_scores.boxplot(ax=ax6)\n",
    "            ax6.set_title('Cross-Validation AUC Scores', fontsize=14, fontweight='bold')\n",
    "            ax6.set_ylabel('AUC Score')\n",
    "            ax6.tick_params(axis='x', rotation=45)\n",
    "            ax6.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def feature_importance_analysis(self):\n",
    "        \"\"\"An√°lise de import√¢ncia das features\"\"\"\n",
    "        \n",
    "        print(f\"\\nüéØ AN√ÅLISE DE IMPORT√ÇNCIA DAS FEATURES:\")\n",
    "        \n",
    "        # Obter melhor modelo\n",
    "        metrics_df = pd.DataFrame({\n",
    "            name: result['metrics'] for name, result in self.final_results.items()\n",
    "        }).T\n",
    "        best_model_name = metrics_df.sort_values('roc_auc', ascending=False).index[0]\n",
    "        best_model = self.final_results[best_model_name]['model']\n",
    "        \n",
    "        print(f\"Analisando import√¢ncias do melhor modelo: {best_model_name}\")\n",
    "        \n",
    "        # Feature importance (para modelos tree-based)\n",
    "        if hasattr(best_model, 'feature_importances_'):\n",
    "            importances = best_model.feature_importances_\n",
    "            feature_names = self.X_train.columns\n",
    "            \n",
    "            # Criar DataFrame\n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': importances\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            # Visualizar top 15\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            top_features = importance_df.head(15)\n",
    "            \n",
    "            plt.barh(range(len(top_features)), top_features['Importance'], color='skyblue')\n",
    "            plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "            plt.xlabel('Feature Importance')\n",
    "            plt.title(f'üèÜ TOP 15 FEATURES MAIS IMPORTANTES - {best_model_name}', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.grid(axis='x', alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"üìä Top 10 Features Mais Importantes:\")\n",
    "            for i, (_, row) in enumerate(importance_df.head(10).iterrows(), 1):\n",
    "                print(f\"{i:2d}. {row['Feature']}: {row['Importance']:.4f}\")\n",
    "        \n",
    "        # SHAP Analysis (se dispon√≠vel)\n",
    "        try:\n",
    "            print(f\"\\nüîç Iniciando an√°lise SHAP...\")\n",
    "            \n",
    "            # Criar explainer SHAP\n",
    "            if best_model_name in ['Random Forest', 'XGBoost', 'Gradient Boosting']:\n",
    "                explainer = shap.TreeExplainer(best_model)\n",
    "                shap_values = explainer.shap_values(self.X_test[:100])  # Amostra para velocidade\n",
    "                \n",
    "                # Se shap_values for lista (classifica√ß√£o bin√°ria), pegar valores da classe 1\n",
    "                if isinstance(shap_values, list):\n",
    "                    shap_values = shap_values[1]\n",
    "                \n",
    "                # Summary plot\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                shap.summary_plot(shap_values, self.X_test[:100], plot_type=\"bar\", show=False)\n",
    "                plt.title(f'üéØ SHAP Feature Importance - {best_model_name}', \n",
    "                         fontsize=16, fontweight='bold', pad=20)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                print(\"‚úÖ An√°lise SHAP conclu√≠da!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è N√£o foi poss√≠vel executar an√°lise SHAP: {e}\")\n",
    "    \n",
    "    def generate_business_insights(self):\n",
    "        \"\"\"Gera insights de neg√≥cio baseados nos modelos\"\"\"\n",
    "        \n",
    "        print(f\"\\nüíº INSIGHTS DE NEG√ìCIO:\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Obter melhor modelo e suas predi√ß√µes\n",
    "        metrics_df = pd.DataFrame({\n",
    "            name: result['metrics'] for name, result in self.final_results.items()\n",
    "        }).T\n",
    "        best_model_name = metrics_df.sort_values('roc_auc', ascending=False).index[0]\n",
    "        best_result = self.final_results[best_model_name]\n",
    "        \n",
    "        # Estat√≠sticas do melhor modelo\n",
    "        best_metrics = best_result['metrics']\n",
    "        probabilities = best_result['probabilities']\n",
    "        predictions = best_result['predictions']\n",
    "        \n",
    "        print(f\"üèÜ MELHOR MODELO: {best_model_name}\")\n",
    "        print(f\"   - AUC-ROC: {best_metrics['roc_auc']:.3f}\")\n",
    "        print(f\"   - Precis√£o: {best_metrics['precision']:.3f}\")\n",
    "        print(f\"   - Recall: {best_metrics['recall']:.3f}\")\n",
    "        print(f\"   - F1-Score: {best_metrics['f1']:.3f}\")\n",
    "        \n",
    "        # An√°lise de segmentos de risco\n",
    "        print(f\"\\nüéØ SEGMENTA√á√ÉO DE RISCO:\")\n",
    "        \n",
    "        # Definir thresholds de risco\n",
    "        high_risk_threshold = 0.7\n",
    "        medium_risk_threshold = 0.3\n",
    "        \n",
    "        high_risk_count = (probabilities > high_risk_threshold).sum()\n",
    "        medium_risk_count = ((probabilities > medium_risk_threshold) & \n",
    "                           (probabilities <= high_risk_threshold)).sum()\n",
    "        low_risk_count = (probabilities <= medium_risk_threshold).sum()\n",
    "        \n",
    "        total_employees = len(probabilities)\n",
    "        \n",
    "        print(f\"   - Alto Risco (>{high_risk_threshold:.1%}): {high_risk_count} funcion√°rios ({high_risk_count/total_employees:.1%})\")\n",
    "        print(f\"   - M√©dio Risco ({medium_risk_threshold:.1%}-{high_risk_threshold:.1%}): {medium_risk_count} funcion√°rios ({medium_risk_count/total_employees:.1%})\")\n",
    "        print(f\"   - Baixo Risco (<{medium_risk_threshold:.1%}): {low_risk_count} funcion√°rios ({low_risk_count/total_employees:.1%})\")\n",
    "        \n",
    "        # ROI potencial\n",
    "        print(f\"\\nüí∞ ESTIMATIVA DE ROI:\")\n",
    "        \n",
    "        # Assumindo custos t√≠picos de RH\n",
    "        cost_replacement = 20000  # Custo m√©dio de substitui√ß√£o\n",
    "        cost_retention_program = 2000  # Custo de programa de reten√ß√£o\n",
    "        \n",
    "        # Funcion√°rios que realmente sa√≠ram e foram identificados corretamente\n",
    "        true_positives = ((self.y_test == 1) & (predictions == 1)).sum()\n",
    "        \n",
    "        # Economia potencial se conseguirmos reter 50% dos identificados\n",
    "        retention_success_rate = 0.5\n",
    "        potential_savings = true_positives * retention_success_rate * cost_replacement\n",
    "        program_costs = high_risk_count * cost_retention_program\n",
    "        net_savings = potential_savings - program_costs\n",
    "        \n",
    "        print(f\"   - Funcion√°rios identificados corretamente para sair: {true_positives}\")\n",
    "        print(f\"   - Economia potencial (50% reten√ß√£o): ${potential_savings:,.0f}\")\n",
    "        print(f\"   - Custos do programa de reten√ß√£o: ${program_costs:,.0f}\")\n",
    "        print(f\"   - Economia l√≠quida estimada: ${net_savings:,.0f}\")\n",
    "        \n",
    "        if net_savings > 0:\n",
    "            roi = (net_savings / program_costs) * 100\n",
    "            print(f\"   - ROI estimado: {roi:.1f}%\")\n",
    "        \n",
    "        # Recomenda√ß√µes espec√≠ficas\n",
    "        print(f\"\\nüìã RECOMENDA√á√ïES DE A√á√ÉO:\")\n",
    "        print(\"   1. üö® PRIORIDADE ALTA - Funcion√°rios com risco > 70%:\")\n",
    "        print(\"      ‚Ä¢ Reuni√£o individual imediata com gestor\")\n",
    "        print(\"      ‚Ä¢ Revis√£o salarial e plano de carreira\")\n",
    "        print(\"      ‚Ä¢ Programa de mentoria personalizado\")\n",
    "        \n",
    "        print(\"   2. ‚ö†Ô∏è PRIORIDADE M√âDIA - Funcion√°rios com risco 30-70%:\")\n",
    "        print(\"      ‚Ä¢ Survey de satisfa√ß√£o direcionado\")\n",
    "        print(\"      ‚Ä¢ Melhorias no ambiente de trabalho\")\n",
    "        print(\"      ‚Ä¢ Programa de desenvolvimento profissional\")\n",
    "        \n",
    "        print(\"   3. ‚úÖ MANUTEN√á√ÉO - Funcion√°rios com risco < 30%:\")\n",
    "        print(\"      ‚Ä¢ Reconhecimento e feedback positivo\")\n",
    "        print(\"      ‚Ä¢ Programa de embaixadores internos\")\n",
    "        print(\"      ‚Ä¢ Manuten√ß√£o das boas pr√°ticas atuais\")\n",
    "        \n",
    "        return {\n",
    "            'best_model': best_model_name,\n",
    "            'metrics': best_metrics,\n",
    "            'risk_segments': {\n",
    "                'high_risk': high_risk_count,\n",
    "                'medium_risk': medium_risk_count,\n",
    "                'low_risk': low_risk_count\n",
    "            },\n",
    "            'roi_analysis': {\n",
    "                'potential_savings': potential_savings,\n",
    "                'program_costs': program_costs,\n",
    "                'net_savings': net_savings\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def run_complete_modeling_pipeline(self, data_dir='processed_data/'):\n",
    "        \"\"\"Executa pipeline completo de modelagem\"\"\"\n",
    "        \n",
    "        print(\"üöÄ EXECUTANDO PIPELINE COMPLETO DE MODELAGEM\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # 1. Carregar dados\n",
    "        if not self.load_processed_data(data_dir):\n",
    "            return None\n",
    "        \n",
    "        # 2. Inicializar modelos\n",
    "        self.initialize_models()\n",
    "        \n",
    "        # 3. Avalia√ß√£o baseline\n",
    "        baseline_results = self.evaluate_baseline_models()\n",
    "        \n",
    "        # 4. Otimiza√ß√£o de hiperpar√¢metros\n",
    "        self.hyperparameter_tuning()\n",
    "        \n",
    "        # 5. Avalia√ß√£o final\n",
    "        final_results = self.evaluate_final_models()\n",
    "        \n",
    "        # 6. Visualiza√ß√µes\n",
    "        self.create_model_comparison_plots()\n",
    "        \n",
    "        # 7. An√°lise de features\n",
    "        self.feature_importance_analysis()\n",
    "        \n",
    "        # 8. Insights de neg√≥cio\n",
    "        business_insights = self.generate_business_insights()\n",
    "        \n",
    "        print(\"\\nüéâ PIPELINE DE MODELAGEM CONCLU√çDO COM SUCESSO!\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return {\n",
    "            'baseline_results': baseline_results,\n",
    "            'final_results': final_results,\n",
    "            'best_models': self.best_models,\n",
    "            'business_insights': business_insights\n",
    "        }\n",
    "    \n",
    "    def save_model_results(self, results, output_dir='model_results/'):\n",
    "        \"\"\"Salva resultados dos modelos\"\"\"\n",
    "        \n",
    "        import os\n",
    "        import joblib\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Salvar melhor modelo\n",
    "        best_model_name = list(results['business_insights']['best_model'])\n",
    "        best_model = self.best_models[best_model_name]\n",
    "        \n",
    "        joblib.dump(best_model, f'{output_dir}best_model.pkl')\n",
    "        \n",
    "        # Salvar resultados\n",
    "        results['baseline_results'].to_csv(f'{output_dir}baseline_results.csv', index=False)\n",
    "        results['final_results'].to_csv(f'{output_dir}final_results.csv')\n",
    "        \n",
    "        print(f\"üíæ Resultados salvos em: {output_dir}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# EXEMPLO DE USO\n",
    "# ========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Inicializar pipeline\n",
    "    modeling_pipeline = AttritionModelingPipeline(random_state=42)\n",
    "    \n",
    "    # Executar pipeline completo\n",
    "    results = modeling_pipeline.run_complete_modeling_pipeline()\n",
    "    \n",
    "    if results:\n",
    "        # Salvar resultados\n",
    "        modeling_pipeline.save_model_results(results)\n",
    "        \n",
    "        print(f\"\\nüìä RESUMO DOS RESULTADOS:\")\n",
    "        print(f\"Melhor modelo: {results['business_insights']['best_model']}\")\n",
    "        \n",
    "        best_metrics = results['business_insights']['metrics']\n",
    "        print(f\"AUC-ROC: {best_metrics['roc_auc']:.3f}\")\n",
    "        print(f\"F1-Score: {best_metrics['f1']:.3f}\")\n",
    "        \n",
    "        roi_analysis = results['business_insights']['roi_analysis']\n",
    "        print(f\"ROI estimado: ${roi_analysis['net_savings']:,.0f}\")\n",
    "        \n",
    "        print(f\"\\nüéØ PR√ìXIMOS PASSOS:\")\n",
    "        print(\"1. Criar dashboard interativo\")\n",
    "        print(\"2. Implementar sistema de monitoramento\")\n",
    "        print(\"3. Desenvolver API para predi√ß√µes em tempo real\")\n",
    "        print(\"4. Criar relat√≥rios executivos automatizados\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
